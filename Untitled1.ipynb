{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "# controllare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810c8a4",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_gpu = True):\n",
    "    \"\"\"\n",
    "    Set SEED for PyTorch reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 44\n",
    "\n",
    "USE_SEED = False\n",
    "\n",
    "if USE_SEED:\n",
    "    set_seed(SEED, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b11b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, device, output_size=None):\n",
    "    \"\"\"Loads an image by transforming it into a tensor.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    output_dim = None\n",
    "    if output_size is None:\n",
    "        output_dim = (img.size[1], img.size[0])\n",
    "    elif isinstance(output_size, int):\n",
    "        output_dim = (output_size, output_size)\n",
    "    elif isinstance(output_size, tuple):\n",
    "        if (len(output_size) == 2) and isinstance(output_size[0], int) and isinstance(output_size[1], int):\n",
    "            output_dim = output_size\n",
    "    else:\n",
    "        raise ValueError(\"ERROR: output_size must be an integer or a 2-tuple of (height, width) if provided.\")\n",
    "\n",
    "    torch_loader = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(output_dim),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    img_tensor = torch_loader(img).unsqueeze(0)\n",
    "    return img_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_style_transfer(config):\n",
    "    \"\"\"Implements neural style transfer on a content image using a style image, applying provided configuration.\"\"\"\n",
    "    ...\n",
    "    \n",
    "    # load content and style images\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_size = config.get('output_image_size')\n",
    "    if output_size is not None:\n",
    "        if len(output_size) > 1: \n",
    "            output_size = tuple(output_size)\n",
    "        else:\n",
    "            output_size = output_size[0]\n",
    "\n",
    "    content_tensor = load_image(content_path, device, output_size=output_size)\n",
    "    output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
    "    style_tensor = load_image(style_path, device, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbce125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gloria´s paths\"\"\"\n",
    "#content_path = \"/home/gloria/Scrivania/Vision_and_cognitive_system/content_style/content.jpg\"\n",
    "#style_path = \"/home/gloria/Scrivania/Vision_and_cognitive_system/content_style/style1.jpg\"\n",
    "\n",
    "\"\"\"Sara´s paths\"\"\"\n",
    "content_path = \"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/home.jpeg\"\n",
    "style_path = \"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/vangogh.jpg\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_size = 512\n",
    "\n",
    "content_tensor = load_image(content_path, device, output_size=output_size)\n",
    "output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
    "style_tensor = load_image(style_path, device, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content_tensor.shape)\n",
    "print(style_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71eb75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = style_tensor.mean(axis=(0,2,3)) \n",
    "train_std = content_tensor.std(axis=(0,2,3))\n",
    "\n",
    "print(\"-----  TRAIN NORMALIZATION VALUES  -----\")\n",
    "print(f\"Mean: {train_mean}\")\n",
    "print(f\"Standard Deviation: {train_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cc13b",
   "metadata": {},
   "source": [
    "# VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "\n",
    "        #select 5 convolutional layers\n",
    "        self.chosen_features = {0: 'conv1_1', 5: 'conv2_1', 10: 'conv3_1', 19: 'conv4_1', 21: 'conv4_2', 28: 'conv5_1'}\n",
    "        self.vgg = torchvision.models.vgg19(pretrained=True).features[:37]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature_maps = dict()\n",
    "        for idx, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            if idx in self.chosen_features.keys():\n",
    "                feature_maps[self.chosen_features[idx]] = x\n",
    "        \n",
    "        return feature_maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "vgg = VGG19().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2416301",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b24cf0",
   "metadata": {},
   "source": [
    "### Structure of VGG\n",
    "The structure is:\n",
    "- Conv1:\n",
    "    - conv 1_1 [0]\n",
    "    - conv 1_2 [2]\n",
    "- Conv2:\n",
    "    - conv 2_1 [5]\n",
    "    - conv 2_2 [7]\n",
    "- Conv3:\n",
    "    - conv 3_1 [10]\n",
    "    - conv 3_2 [12]\n",
    "    - conv 3_3 [14]\n",
    "    - conv 3_4 [16]\n",
    "- Conv4:\n",
    "    - conv 4_1 [19]\n",
    "    - conv 4_2 [21]\n",
    "    - conv 4_3 [23]\n",
    "    - conv 4_4 [25]\n",
    "- Conv5:\n",
    "    - conv 5_1 [28]\n",
    "    - conv 5_2 [30]\n",
    "    - conv 5_3 [32]\n",
    "    - conv 5_4 [34]\n",
    "    \n",
    "We will use:\n",
    "- For content loss: conv4_2 \n",
    "- For style loss: conv1_1,conv2_1,conv3_1,conv4_1,conv5_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248352ba",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d21ea",
   "metadata": {},
   "source": [
    "The overall loss is constituted by the loss of the target image with respect to the content image, and the loss of the target image with respect to the style image. $$L_{tot}=L_{content}+L_{style}$$\n",
    "For this process it wouldn´t make sense to compare the images pixel by pixel: for example if the content image contains a house and the predominant style of the style image is to have diagonal lines, we would want the target image to be a house which is inclinated diagonally; comparing pixel by pixel an image with a diagonal house and an image with a house would return a much higher loss than we expect, because the pixel by pixel comparison doesn´t take into account more ǵeneric'features\n",
    "In order to perform a more accurate comparison, both these losses are evaluated between **feature maps** which take into account the more generic features of both images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f19ad7",
   "metadata": {},
   "source": [
    "### Content loss\n",
    "The content loss is computed at the end of the CNN; we compute the mean squared error between the target feature map and the content feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(target_map, content_map):\n",
    "    #return torch.mean((content_original-content_current)**2)\n",
    "    return torch.nn.MSELoss(reduction='mean')(target_map, content_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ba0be",
   "metadata": {},
   "source": [
    "### Style loss\n",
    "For the style loss, the procedure is more complicated.\n",
    "We are interested in co-occurrences of pairs of features to highlight important stylistic combinations.    \n",
    "\n",
    "If we have a feature map, of height and width $h,w$ and lenght $k$, which is the number of maps applied, we want to compute cooccurrences between each pair of  maps $i,j$ with values in $[0,k]$ range (these are also called *channels*): we obtain a $kxk$ matrix in which each entry is the dot product between two maps, a scalar.    \n",
    "Given the feature map of an image, this matrix, called the **Gram matrix** can be computed easily as the sum of the matrix multiplication between the whole feature map and its transpose.\n",
    "   \n",
    "This is done both with the feature map of the STYLE IMAGE and the feature map of the TARGET IMAGE.\n",
    "We compute a Gram matrix for both images for each convolutional layer considered $l$, and end up with:\n",
    "   - 5 Gram matrices of the style image feature maps $G_{style}^l$\n",
    "   - 5 Gram matrices of the target image feature maps  $G_{target}^l$    \n",
    "   \n",
    "The loss of each layer $l$ is computed via MSE between the two gram matrices, and the overall style loss will be the average of these values over the number of layers (in our case 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(target_map,style_map):\n",
    "    \"\"\"Compute MSE between gram matrix of style feature map and of generated feature map as style loss.\"\"\"\n",
    "    _, channel, height, width = target_map.shape\n",
    "    \n",
    "    #computing Gram matrix of the style feature map\n",
    "    style_gram = style_map.view(channel, height*width).mm(\n",
    "        style_map.view(channel, height*width).t()\n",
    "    )\n",
    "    #computing Gram matrix of the target feature map\n",
    "    target_gram = target_map.view(channel, height*width).mm(\n",
    "        target_map.view(channel, height*width).t()\n",
    "    )\n",
    "\n",
    "    return torch.nn.MSELoss(reduction='sum')(target_gram,style_gram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33194d08",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddae890",
   "metadata": {},
   "source": [
    "### Initialize random (target) image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=content_tensor\n",
    "img.shape         #to take the right dimensions for the generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91646e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Generating random noise as input image '\n",
    "#gaussian_noise_img = np.random.normal(loc=0, scale=90., size=img.shape).astype(np.float32)\n",
    "white_noise_img = np.random.uniform(-90., 90., img.shape).astype(np.float32)\n",
    "init_img = torch.from_numpy(white_noise_img).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_img = (init_img - init_img.min()) / (init_img.max() - init_img.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d488807",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2895eb",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85387742",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = ['conv1_1','conv2_1','conv3_1','conv4_1','conv5_1']\n",
    "content_layers = ['conv4_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = content_tensor\n",
    "style = style_tensor\n",
    "target = init_img.requires_grad_(True)  #requires_grad is needed to make sure that the image is updated\n",
    "\n",
    "\n",
    "learn_rate=1e1\n",
    "alpha=1\n",
    "beta=1000\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "intermediate_dir=\"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/intermediate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05029f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image(content, style, target, device, output_img_fmt, content_img_name, style_img_name, num_epochs):\n",
    "    \"\"\"Update the output image using pre-trained VGG19 model.\"\"\"\n",
    "    ...\n",
    "    \n",
    "    model = VGG19().to(device).eval()    # freeze parameters in the model\n",
    "    optimizer = torch.optim.Adam([target], lr=learn_rate)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # get features maps of content, style and generated images from chosen layers\n",
    "        content_features = model(content)\n",
    "        style_features = model(style)\n",
    "        target_features = model(target)\n",
    "\n",
    "        content_loss  = 0.0\n",
    "        style_loss= 0.0\n",
    "        \n",
    "        \"\"\"Computing loss\"\"\"\n",
    "        for layer in target_features.keys(): \n",
    "            content_feature = content_features[layer]\n",
    "            style_feature = style_features[layer]\n",
    "            target_feature = target_features[layer]\n",
    "   \n",
    "            \n",
    "            if layer in content_layers:\n",
    "            # computes content loss on layer 4_2\n",
    "                content_loss_per_feature = get_content_loss(content_feature, target_feature)\n",
    "                content_loss += content_loss_per_feature\n",
    "                \n",
    "            if layer in style_layers:\n",
    "            # computes style loss for all 5 style layers\n",
    "                style_loss_per_feature = get_style_loss(style_feature, target_feature)\n",
    "                style_loss+=style_loss_per_feature\n",
    "        \n",
    "        #average style loss over all 5 layers\n",
    "        style_loss /= len(style_layers)\n",
    "        \n",
    "        # Total loss \n",
    "        total_loss = alpha * content_loss + beta * style_loss\n",
    "\n",
    "        #optimizer.zero_grad()\n",
    "      #compute the gradient\n",
    "        total_loss.backward()\n",
    "      #update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Save every 100 steps\n",
    "        if ((epoch+1)%10)==0:\n",
    "            save_image(target, os.path.join(intermediate_dir, f'nst-{content_img_name}-{style_img_name}-{epoch + 1}.{output_img_fmt}'))\n",
    "\n",
    "        print(f\"\\tEpoch {epoch + 1}/{num_epochs}, loss = {total_loss}\")\n",
    "    ...\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image(content, style, target, device,'jpeg', 'house', 'vangogh',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_image(content, os.path.join(intermediate_dir, 'content_after.jpg'))\n",
    "#save_image(style, os.path.join(intermediate_dir, 'style_after.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b5363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
