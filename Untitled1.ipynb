{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80dc6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "# controllare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810c8a4",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbf120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_gpu = True):\n",
    "    \"\"\"\n",
    "    Set SEED for PyTorch reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 44\n",
    "\n",
    "USE_SEED = False\n",
    "\n",
    "if USE_SEED:\n",
    "    set_seed(SEED, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b11b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, device, output_size=None):\n",
    "    \"\"\"Loads an image by transforming it into a tensor.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    output_dim = None\n",
    "    if output_size is None:\n",
    "        output_dim = (img.size[1], img.size[0])\n",
    "    elif isinstance(output_size, int):\n",
    "        output_dim = (output_size, output_size)\n",
    "    elif isinstance(output_size, tuple):\n",
    "        if (len(output_size) == 2) and isinstance(output_size[0], int) and isinstance(output_size[1], int):\n",
    "            output_dim = output_size\n",
    "    else:\n",
    "        raise ValueError(\"ERROR: output_size must be an integer or a 2-tuple of (height, width) if provided.\")\n",
    "\n",
    "    torch_loader = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(output_dim),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    img_tensor = torch_loader(img).unsqueeze(0)\n",
    "    return img_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f1a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_style_transfer(config):\n",
    "    \"\"\"Implements neural style transfer on a content image using a style image, applying provided configuration.\"\"\"\n",
    "    ...\n",
    "    \n",
    "    # load content and style images\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_size = config.get('output_image_size')\n",
    "    if output_size is not None:\n",
    "        if len(output_size) > 1: \n",
    "            output_size = tuple(output_size)\n",
    "        else:\n",
    "            output_size = output_size[0]\n",
    "\n",
    "    content_tensor = load_image(content_path, device, output_size=output_size)\n",
    "    output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
    "    style_tensor = load_image(style_path, device, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbce125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gloria´s paths\"\"\"\n",
    "#content_path = \"/home/gloria/Scrivania/Vision_and_cognitive_system/content_style/content.jpg\"\n",
    "#style_path = \"/home/gloria/Scrivania/Vision_and_cognitive_system/content_style/style1.jpg\"\n",
    "\n",
    "\"\"\"Sara´s paths\"\"\"\n",
    "content_path = \"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/home.jpeg\"\n",
    "style_path = \"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/vangogh.jpeg\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_size = 512\n",
    "\n",
    "content_tensor = load_image(content_path, device, output_size=output_size)\n",
    "output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
    "style_tensor = load_image(style_path, device, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd136a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(content_tensor.shape)\n",
    "print(style_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71eb75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  TRAIN NORMALIZATION VALUES  -----\n",
      "Mean: tensor([0.5932, 0.5963, 0.5475])\n",
      "Standard Deviation: tensor([0.2506, 0.2537, 0.2901])\n"
     ]
    }
   ],
   "source": [
    "train_mean = style_tensor.mean(axis=(0,2,3)) \n",
    "train_std = content_tensor.std(axis=(0,2,3))\n",
    "\n",
    "print(\"-----  TRAIN NORMALIZATION VALUES  -----\")\n",
    "print(f\"Mean: {train_mean}\")\n",
    "print(f\"Standard Deviation: {train_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cc13b",
   "metadata": {},
   "source": [
    "# VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5902904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "\n",
    "        #select 5 convolutional layers\n",
    "        self.chosen_features = {0: 'conv1_1', 5: 'conv2_1', 10: 'conv3_1', 19: 'conv4_1', 28: 'conv5_1'}\n",
    "        self.vgg = torchvision.models.vgg19(pretrained=True).features[:37]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature_maps = dict()\n",
    "        for idx, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            if idx in self.chosen_features.keys():\n",
    "                feature_maps[self.chosen_features[idx]] = x\n",
    "        \n",
    "        return feature_maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707a0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sara/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sara/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "vgg = VGG19().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2416301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG19(\n",
       "  (vgg): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248352ba",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d21ea",
   "metadata": {},
   "source": [
    "The overall loss is constituted by the loss of the target image with respect to the content image, and the loss of the target image with respect to the style image. $$L_{tot}=L_{content}+L_{style}$$\n",
    "For this process it wouldn´t make sense to compare the images pixel by pixel: for example if the content image contains a house and the predominant style of the style image is to have diagonal lines, we would want the target image to be a house which is inclinated diagonally; comparing pixel by pixel an image with a diagonal house and an image with a house would return a much higher loss than we expect, because the pixel by pixel comparison doesn´t take into account more ǵeneric'features\n",
    "In order to perform a more accurate comparison, both these losses are evaluated between **feature maps** which take into account the more generic features of both images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f19ad7",
   "metadata": {},
   "source": [
    "### Content loss\n",
    "The content loss is computed at the end of the CNN; we compute the mean squared error between the target feature map and the content feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995d65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(target_map, content_map):\n",
    "    #return torch.mean((content_original-content_current)**2)\n",
    "    return torch.nn.MSELoss(reduction='mean')(target_map, content_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ba0be",
   "metadata": {},
   "source": [
    "### Style loss\n",
    "For the style loss, the procedure is more complicated.\n",
    "We are interested in co-occurrences of pairs of features to highlight important stylistic combinations.    \n",
    "\n",
    "If we have a feature map, of height and width $h,w$ and lenght $k$, which is the number of maps applied, we want to compute cooccurrences between each pair of  maps $i,j$ with values in $[0,k]$ range (these are also called *channels*): we obtain a $kxk$ matrix in which each entry is the dot product between two maps, a scalar.    \n",
    "Given the feature map of an image, this matrix, called the **Gram matrix** can be computed easily as the sum of the matrix multiplication between the whole feature map and its transpose.\n",
    "   \n",
    "This is done both with the feature map of the STYLE IMAGE and the feature map of the TARGET IMAGE.\n",
    "We compute a Gram matrix for both images for each convolutional layer considered $l$, and end up with:\n",
    "   - 5 Gram matrices of the style image feature maps $G_{style}^l$\n",
    "   - 5 Gram matrices of the target image feature maps  $G_{target}^l$    \n",
    "   \n",
    "The loss of each layer $l$ is computed via MSE between the two gram matrices, and the overall style loss will be the average of these values over the number of layers (in our case 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ae8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(target_map,style_map):\n",
    "    \"\"\"Compute MSE between gram matrix of style feature map and of generated feature map as style loss.\"\"\"\n",
    "    _, channel, height, width = target_map.shape\n",
    "    \n",
    "    #computing Gram matrix of the style feature map\n",
    "    style_gram = style_map.view(channel, height*width).mm(\n",
    "        style_map.view(channel, height*width).t()\n",
    "    )\n",
    "    #computing Gram matrix of the target feature map\n",
    "    target_gram = target_map.view(channel, height*width).mm(\n",
    "        target_map.view(channel, height*width).t()\n",
    "    )\n",
    "\n",
    "    return torch.nn.MSELoss(reduction='sum')(target_gram,style_gram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33194d08",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddae890",
   "metadata": {},
   "source": [
    "### Initialize random (target) image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2cc5ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=content_tensor\n",
    "img.shape         #to take the right dimensions for the generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91646e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Generating random noise as input image '\n",
    "#gaussian_noise_img = np.random.normal(loc=0, scale=90., size=img.shape).astype(np.float32)\n",
    "white_noise_img = np.random.uniform(-90., 90., img.shape).astype(np.float32)\n",
    "init_img = torch.from_numpy(white_noise_img).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2895eb",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2ba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = content_tensor\n",
    "style = style_tensor\n",
    "target = init_img.requires_grad_(True)  #requires_grad is needed to make sure that the image is updated\n",
    "\n",
    "\n",
    "num_epochs=200\n",
    "learn_rate=1e1\n",
    "alpha=1\n",
    "beta=0.01\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "intermediate_dir=\"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/intermediate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05029f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image(content, style, target, device, output_img_fmt, content_img_name, style_img_name):\n",
    "    \"\"\"Update the output image using pre-trained VGG19 model.\"\"\"\n",
    "    ...\n",
    "    \n",
    "    model = VGG19().to(device).eval()    # freeze parameters in the model\n",
    "    optimizer = torch.optim.Adam([target], lr=learn_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # get features maps of content, style and generated images from chosen layers\n",
    "        content_features = model(content)\n",
    "        style_features = model(style)\n",
    "        target_features = model(target)\n",
    "\n",
    "        content_loss  = 0.0\n",
    "        style_loss= 0.0\n",
    "        \n",
    "        \"\"\"Computing loss\"\"\"\n",
    "        for i,layer_name in enumerate(target_features.keys()): \n",
    "            content_feature = content_features[layer_name]\n",
    "            style_feature = style_features[layer_name]\n",
    "            target_feature = target_features[layer_name]\n",
    "   \n",
    "            \n",
    "            if layer_name==\"conv5_1\":\n",
    "                # computes content loss on the last convolutional layer\n",
    "                content_loss_per_feature = get_content_loss(content_feature, target_feature)\n",
    "                content_loss += content_loss_per_feature\n",
    "            \n",
    "            \n",
    "            # computes style loss for all selected layers\n",
    "            style_loss_per_feature = get_style_loss(style_feature, target_feature)\n",
    "            style_loss+=style_loss_per_feature\n",
    "        \n",
    "        #average style loss over all 5 layers\n",
    "        style_loss /= len(target_features.keys())\n",
    "        \n",
    "        # Total loss \n",
    "        total_loss = alpha * content_loss + beta * style_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "      #compute the gradient\n",
    "        total_loss.backward()\n",
    "      #update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        save_image(target, os.path.join(intermediate_dir, f'nst-{content_img_name}-{style_img_name}-{epoch + 1}.{output_img_fmt}'))\n",
    "\n",
    "        print(f\"\\tEpoch {epoch + 1}/{num_epochs}, loss = {total_loss}\")\n",
    "    ...\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c3f894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1/200, loss = 7.306305344106398e+19\n",
      "\tEpoch 2/200, loss = 2.0706625297858953e+19\n",
      "\tEpoch 3/200, loss = 1.088128933649449e+19\n",
      "\tEpoch 4/200, loss = 7.757765368227037e+18\n",
      "\tEpoch 5/200, loss = 6.436997366326231e+18\n",
      "\tEpoch 6/200, loss = 5.70315526840202e+18\n",
      "\tEpoch 7/200, loss = 5.205017127391592e+18\n",
      "\tEpoch 8/200, loss = 4.839024341121237e+18\n",
      "\tEpoch 9/200, loss = 4.550516613161419e+18\n",
      "\tEpoch 10/200, loss = 4.3109027928992645e+18\n",
      "\tEpoch 11/200, loss = 4.093526045553066e+18\n",
      "\tEpoch 12/200, loss = 3.8815330568922726e+18\n",
      "\tEpoch 13/200, loss = 3.6627951141508874e+18\n",
      "\tEpoch 14/200, loss = 3.435400716364022e+18\n",
      "\tEpoch 15/200, loss = 3.201808096653476e+18\n",
      "\tEpoch 16/200, loss = 2.966257246733861e+18\n",
      "\tEpoch 17/200, loss = 2.7331135026453545e+18\n",
      "\tEpoch 18/200, loss = 2.5065701268583875e+18\n",
      "\tEpoch 19/200, loss = 2.2903227153928684e+18\n",
      "\tEpoch 20/200, loss = 2.0874143041176207e+18\n",
      "\tEpoch 21/200, loss = 1.8999171975730954e+18\n",
      "\tEpoch 22/200, loss = 1.727603247096529e+18\n",
      "\tEpoch 23/200, loss = 1.5703159097199165e+18\n",
      "\tEpoch 24/200, loss = 1.4274838517136753e+18\n",
      "\tEpoch 25/200, loss = 1.2982451934005821e+18\n",
      "\tEpoch 26/200, loss = 1.1812281565860332e+18\n",
      "\tEpoch 27/200, loss = 1.0752831331437117e+18\n",
      "\tEpoch 28/200, loss = 9.796583319981261e+17\n",
      "\tEpoch 29/200, loss = 8.934619117801964e+17\n",
      "\tEpoch 30/200, loss = 8.156872695946936e+17\n",
      "\tEpoch 31/200, loss = 7.454383721844572e+17\n",
      "\tEpoch 32/200, loss = 6.820187476527677e+17\n",
      "\tEpoch 33/200, loss = 6.247777599938888e+17\n",
      "\tEpoch 34/200, loss = 5.7310212223768986e+17\n",
      "\tEpoch 35/200, loss = 5.2642860955284275e+17\n",
      "\tEpoch 36/200, loss = 4.843154587831501e+17\n",
      "\tEpoch 37/200, loss = 4.4632499558128026e+17\n",
      "\tEpoch 38/200, loss = 4.119982082023752e+17\n",
      "\tEpoch 39/200, loss = 3.809490993456087e+17\n",
      "\tEpoch 40/200, loss = 3.528132152661115e+17\n",
      "\tEpoch 41/200, loss = 3.273099056208937e+17\n",
      "\tEpoch 42/200, loss = 3.041701336585339e+17\n",
      "\tEpoch 43/200, loss = 2.8314090862542848e+17\n",
      "\tEpoch 44/200, loss = 2.6399378980103782e+17\n",
      "\tEpoch 45/200, loss = 2.465435781955584e+17\n",
      "\tEpoch 46/200, loss = 2.306064663885906e+17\n",
      "\tEpoch 47/200, loss = 2.1604355513778176e+17\n",
      "\tEpoch 48/200, loss = 2.027385539086254e+17\n",
      "\tEpoch 49/200, loss = 1.9056409115099136e+17\n",
      "\tEpoch 50/200, loss = 1.7941198522862797e+17\n",
      "\tEpoch 51/200, loss = 1.691898256251945e+17\n",
      "\tEpoch 52/200, loss = 1.5980248740501914e+17\n",
      "\tEpoch 53/200, loss = 1.511698608380969e+17\n",
      "\tEpoch 54/200, loss = 1.4322605253617254e+17\n",
      "\tEpoch 55/200, loss = 1.3590581335608525e+17\n",
      "\tEpoch 56/200, loss = 1.2915323141357568e+17\n",
      "\tEpoch 57/200, loss = 1.229178837925888e+17\n",
      "\tEpoch 58/200, loss = 1.1714975989392998e+17\n",
      "\tEpoch 59/200, loss = 1.1180904537076531e+17\n",
      "\tEpoch 60/200, loss = 1.0685722295638426e+17\n",
      "\tEpoch 61/200, loss = 1.0226084344548557e+17\n",
      "\tEpoch 62/200, loss = 9.798931808098714e+16\n",
      "\tEpoch 63/200, loss = 9.401249619247104e+16\n",
      "\tEpoch 64/200, loss = 9.030595659589222e+16\n",
      "\tEpoch 65/200, loss = 8.684583645295411e+16\n",
      "\tEpoch 66/200, loss = 8.361115914338304e+16\n",
      "\tEpoch 67/200, loss = 8.058380849512448e+16\n",
      "\tEpoch 68/200, loss = 7.774915584956826e+16\n",
      "\tEpoch 69/200, loss = 7.509043365439078e+16\n",
      "\tEpoch 70/200, loss = 7.259403545319834e+16\n",
      "\tEpoch 71/200, loss = 7.024723096292557e+16\n",
      "\tEpoch 72/200, loss = 6.803836793729843e+16\n",
      "\tEpoch 73/200, loss = 6.595671754801152e+16\n",
      "\tEpoch 74/200, loss = 6.399235412564378e+16\n",
      "\tEpoch 75/200, loss = 6.213630548351386e+16\n",
      "\tEpoch 76/200, loss = 6.038152787525632e+16\n",
      "\tEpoch 77/200, loss = 5.872037196411699e+16\n",
      "\tEpoch 78/200, loss = 5.71455062409216e+16\n",
      "\tEpoch 79/200, loss = 5.56508576219136e+16\n",
      "\tEpoch 80/200, loss = 5.423066655594906e+16\n",
      "\tEpoch 81/200, loss = 5.287995088096461e+16\n",
      "\tEpoch 82/200, loss = 5.159409350711706e+16\n",
      "\tEpoch 83/200, loss = 5.036882523691418e+16\n",
      "\tEpoch 84/200, loss = 4.919945167110144e+16\n",
      "\tEpoch 85/200, loss = 4.808221900826214e+16\n",
      "\tEpoch 86/200, loss = 4.701381153364378e+16\n",
      "\tEpoch 87/200, loss = 4.599129578458317e+16\n",
      "\tEpoch 88/200, loss = 4.501133027652403e+16\n",
      "\tEpoch 89/200, loss = 4.407164297176678e+16\n",
      "\tEpoch 90/200, loss = 4.316977714901811e+16\n",
      "\tEpoch 91/200, loss = 4.230334480646144e+16\n",
      "\tEpoch 92/200, loss = 4.147012544600474e+16\n",
      "\tEpoch 93/200, loss = 4.066852992974848e+16\n",
      "\tEpoch 94/200, loss = 3.989652673816166e+16\n",
      "\tEpoch 95/200, loss = 3.91524880786391e+16\n",
      "\tEpoch 96/200, loss = 3.843482910824858e+16\n",
      "\tEpoch 97/200, loss = 3.774199934379622e+16\n",
      "\tEpoch 98/200, loss = 3.707274035986432e+16\n",
      "\tEpoch 99/200, loss = 3.642582379580621e+16\n",
      "\tEpoch 100/200, loss = 3.5800105042837504e+16\n",
      "\tEpoch 101/200, loss = 3.519459196351283e+16\n",
      "\tEpoch 102/200, loss = 3.4608112031760384e+16\n",
      "\tEpoch 103/200, loss = 3.403984061385933e+16\n",
      "\tEpoch 104/200, loss = 3.3488841406939136e+16\n",
      "\tEpoch 105/200, loss = 3.295440144642867e+16\n",
      "\tEpoch 106/200, loss = 3.243558442945741e+16\n",
      "\tEpoch 107/200, loss = 3.193164303171584e+16\n",
      "\tEpoch 108/200, loss = 3.1441849256247296e+16\n",
      "\tEpoch 109/200, loss = 3.096557603782656e+16\n",
      "\tEpoch 110/200, loss = 3.050229939044352e+16\n",
      "\tEpoch 111/200, loss = 3.0051493180604416e+16\n",
      "\tEpoch 112/200, loss = 2.9612622684880896e+16\n",
      "\tEpoch 113/200, loss = 2.918529276628173e+16\n",
      "\tEpoch 114/200, loss = 2.8768889244483584e+16\n",
      "\tEpoch 115/200, loss = 2.836299550765875e+16\n",
      "\tEpoch 116/200, loss = 2.7967201386430464e+16\n",
      "\tEpoch 117/200, loss = 2.7581094563938304e+16\n",
      "\tEpoch 118/200, loss = 2.720427775570739e+16\n",
      "\tEpoch 119/200, loss = 2.6836486821249024e+16\n",
      "\tEpoch 120/200, loss = 2.6477384605630464e+16\n",
      "\tEpoch 121/200, loss = 2.6126625363984384e+16\n",
      "\tEpoch 122/200, loss = 2.578389341621453e+16\n",
      "\tEpoch 123/200, loss = 2.544892676931584e+16\n",
      "\tEpoch 124/200, loss = 2.512144839789773e+16\n",
      "\tEpoch 125/200, loss = 2.480123711114445e+16\n",
      "\tEpoch 126/200, loss = 2.448798152392704e+16\n",
      "\tEpoch 127/200, loss = 2.418143682310963e+16\n",
      "\tEpoch 128/200, loss = 2.3881450537353216e+16\n",
      "\tEpoch 129/200, loss = 2.358780791829299e+16\n",
      "\tEpoch 130/200, loss = 2.33003049549824e+16\n",
      "\tEpoch 131/200, loss = 2.301873548899123e+16\n",
      "\tEpoch 132/200, loss = 2.274289550937293e+16\n",
      "\tEpoch 133/200, loss = 2.2472606774984704e+16\n",
      "\tEpoch 134/200, loss = 2.2207776944029696e+16\n",
      "\tEpoch 135/200, loss = 2.1948167645822976e+16\n",
      "\tEpoch 136/200, loss = 2.169363499896013e+16\n",
      "\tEpoch 137/200, loss = 2.144408236667699e+16\n",
      "\tEpoch 138/200, loss = 2.119928426319053e+16\n",
      "\tEpoch 139/200, loss = 2.0959101102063616e+16\n",
      "\tEpoch 140/200, loss = 2.072344054149939e+16\n",
      "\tEpoch 141/200, loss = 2.049217158499533e+16\n",
      "\tEpoch 142/200, loss = 2.026519115333632e+16\n",
      "\tEpoch 143/200, loss = 2.004239831479091e+16\n",
      "\tEpoch 144/200, loss = 1.982364918795469e+16\n",
      "\tEpoch 145/200, loss = 1.960883639864525e+16\n",
      "\tEpoch 146/200, loss = 1.939785686764749e+16\n",
      "\tEpoch 147/200, loss = 1.919059248336077e+16\n",
      "\tEpoch 148/200, loss = 1.898695305147187e+16\n",
      "\tEpoch 149/200, loss = 1.8786867705020416e+16\n",
      "\tEpoch 150/200, loss = 1.859020544750387e+16\n",
      "\tEpoch 151/200, loss = 1.839691044434739e+16\n",
      "\tEpoch 152/200, loss = 1.820686887891763e+16\n",
      "\tEpoch 153/200, loss = 1.802003994902528e+16\n",
      "\tEpoch 154/200, loss = 1.7836315206746112e+16\n",
      "\tEpoch 155/200, loss = 1.7655639891247104e+16\n",
      "\tEpoch 156/200, loss = 1.747796138917888e+16\n",
      "\tEpoch 157/200, loss = 1.730322601345024e+16\n",
      "\tEpoch 158/200, loss = 1.7131369339551744e+16\n",
      "\tEpoch 159/200, loss = 1.6962325869232128e+16\n",
      "\tEpoch 160/200, loss = 1.6795977490890752e+16\n",
      "\tEpoch 161/200, loss = 1.6632265148727296e+16\n",
      "\tEpoch 162/200, loss = 1.6471170589130752e+16\n",
      "\tEpoch 163/200, loss = 1.6312589659144192e+16\n",
      "\tEpoch 164/200, loss = 1.6156509473865728e+16\n",
      "\tEpoch 165/200, loss = 1.6002868830011392e+16\n",
      "\tEpoch 166/200, loss = 1.5851617261715456e+16\n",
      "\tEpoch 167/200, loss = 1.5702682828275712e+16\n",
      "\tEpoch 168/200, loss = 1.5556016137568256e+16\n",
      "\tEpoch 169/200, loss = 1.5411587124822016e+16\n",
      "\tEpoch 170/200, loss = 1.5269335660494848e+16\n",
      "\tEpoch 171/200, loss = 1.512919409885184e+16\n",
      "\tEpoch 172/200, loss = 1.4991117342736384e+16\n",
      "\tEpoch 173/200, loss = 1.4855084991053824e+16\n",
      "\tEpoch 174/200, loss = 1.4721056241614848e+16\n",
      "\tEpoch 175/200, loss = 1.4589009619582976e+16\n",
      "\tEpoch 176/200, loss = 1.4458874257997824e+16\n",
      "\tEpoch 177/200, loss = 1.4330601838477312e+16\n",
      "\tEpoch 178/200, loss = 1.4204163369992192e+16\n",
      "\tEpoch 179/200, loss = 1.4079528787771392e+16\n",
      "\tEpoch 180/200, loss = 1.3956679838203904e+16\n",
      "\tEpoch 181/200, loss = 1.3835557465489408e+16\n",
      "\tEpoch 182/200, loss = 1.3716132678598656e+16\n",
      "\tEpoch 183/200, loss = 1.3598402256306176e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 184/200, loss = 1.3482289962942464e+16\n",
      "\tEpoch 185/200, loss = 1.33677904297984e+16\n",
      "\tEpoch 186/200, loss = 1.3254863928426496e+16\n",
      "\tEpoch 187/200, loss = 1.3143487910248448e+16\n",
      "\tEpoch 188/200, loss = 1.3033626941784064e+16\n",
      "\tEpoch 189/200, loss = 1.2925250958262272e+16\n",
      "\tEpoch 190/200, loss = 1.281832452620288e+16\n",
      "\tEpoch 191/200, loss = 1.271284012941312e+16\n",
      "\tEpoch 192/200, loss = 1.260875696570368e+16\n",
      "\tEpoch 193/200, loss = 1.2506043896561664e+16\n",
      "\tEpoch 194/200, loss = 1.240468159463424e+16\n",
      "\tEpoch 195/200, loss = 1.2304655027535872e+16\n",
      "\tEpoch 196/200, loss = 1.220594272043008e+16\n",
      "\tEpoch 197/200, loss = 1.210851675602944e+16\n",
      "\tEpoch 198/200, loss = 1.2012332037177344e+16\n",
      "\tEpoch 199/200, loss = 1.1917377826455552e+16\n",
      "\tEpoch 200/200, loss = 1.1823656271347712e+16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image(content, style, target, device,'jpeg', 'house', 'vangogh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ac935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
