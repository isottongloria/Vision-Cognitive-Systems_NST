{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80dc6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "# controllare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810c8a4",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbf120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_gpu = True):\n",
    "    \"\"\"\n",
    "    Set SEED for PyTorch reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 44\n",
    "\n",
    "USE_SEED = False\n",
    "\n",
    "if USE_SEED:\n",
    "    set_seed(SEED, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71b11b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, device, output_size=None):\n",
    "    \"\"\"Loads an image by transforming it into a tensor.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    output_dim = None\n",
    "    if output_size is None:\n",
    "        output_dim = (img.size[1], img.size[0])\n",
    "    elif isinstance(output_size, int):\n",
    "        output_dim = (output_size, output_size)\n",
    "    elif isinstance(output_size, tuple):\n",
    "        if (len(output_size) == 2) and isinstance(output_size[0], int) and isinstance(output_size[1], int):\n",
    "            output_dim = output_size\n",
    "    else:\n",
    "        raise ValueError(\"ERROR: output_size must be an integer or a 2-tuple of (height, width) if provided.\")\n",
    "\n",
    "    torch_loader = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(output_dim),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    img_tensor = torch_loader(img).unsqueeze(0)\n",
    "    return img_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76f1a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_style_transfer(config):\n",
    "    \"\"\"Implements neural style transfer on a content image using a style image, applying provided configuration.\"\"\"\n",
    "    ...\n",
    "    \n",
    "    # load content and style images\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_size = config.get('output_image_size')\n",
    "    if output_size is not None:\n",
    "        if len(output_size) > 1: \n",
    "            output_size = tuple(output_size)\n",
    "        else:\n",
    "            output_size = output_size[0]\n",
    "\n",
    "    content_tensor = load_image(content_path, device, output_size=output_size)\n",
    "    output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
    "    style_tensor = load_image(style_path, device, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5dbce125",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = \"/home/gloria/Scrivania/Vision_and_cognitive_system/content_style/content.jpg\"\n",
    "style_path = \"/home/gloria/Scrivania/Vision_and_cognitive_system/content_style/style1.jpg\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_size = 32\n",
    "\n",
    "content_tensor = load_image(content_path, device, output_size=output_size)\n",
    "#output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
    "style_tensor = load_image(style_path, device, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6bd136a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_tensor.shape\n",
    "style_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d71eb75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  TRAIN NORMALIZATION VALUES  -----\n",
      "Mean: tensor([0.5936, 0.5965, 0.5478])\n",
      "Standard Deviation: tensor([0.2215, 0.2203, 0.2530])\n"
     ]
    }
   ],
   "source": [
    "train_mean = style_tensor.mean(axis=(0,2,3)) \n",
    "train_std = content_tensor.std(axis=(0,2,3))\n",
    "\n",
    "print(\"-----  TRAIN NORMALIZATION VALUES  -----\")\n",
    "print(f\"Mean: {train_mean}\")\n",
    "print(f\"Standard Deviation: {train_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cc13b",
   "metadata": {},
   "source": [
    "# VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f1d380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "vgg19 = torchvision.models.vgg19(pretrained = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27e43a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5902904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "\n",
    "        self.chosen_features = {0: 'conv1_1', 5: 'conv2_1', 10: 'conv3_1', 19: 'conv4_1', 28: 'conv5_1'}\n",
    "        self.vgg = torchvision.models.vgg19(pretrained=True).features[:29]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature_maps = dict()\n",
    "        for idx, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            if idx in self.chosen_features.keys():\n",
    "                feature_maps[self.chosen_features[idx]] = x\n",
    "        \n",
    "        return feature_maps\n",
    "\n",
    "#load the model\n",
    "vgg = VGG19().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2416301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG19(\n",
       "  (vgg): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248352ba",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "995d65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(content_original, content_current):\n",
    "    #return torch.mean((content_original-content_current)**2)\n",
    "    return torch.nn.MSELoss(reduction='mean')(content_original, content_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "88fd5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x, should_normalize=True):\n",
    "    (ch, h, w) = x.size()\n",
    "    features = x.view(ch, w * h)\n",
    "    features_t = features.t()\n",
    "    gram = features.mm(features_t)\n",
    "    if should_normalize:\n",
    "        gram /= ch * h * w\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1729aaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1309, 0.1274, 0.1070],\n",
       "        [0.1274, 0.1301, 0.1151],\n",
       "        [0.1070, 0.1151, 0.1123]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_matrix1 = gram_matrix(style_tensor, should_normalize=True)\n",
    "gram_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e22cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_loss = 0.0\n",
    "    current_style_representation = [utils.gram_matrix(x) for cnt, x in enumerate(current_set_of_feature_maps) if cnt in style_feature_maps_indices]\n",
    "    for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):\n",
    "        style_loss += torch.nn.MSELoss(reduction='sum')(gram_gt[0], gram_hat[0])\n",
    "    style_loss /= len(target_style_representation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
