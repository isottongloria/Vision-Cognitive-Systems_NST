{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f8bf2b",
   "metadata": {},
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810c8a4",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_gpu = True):\n",
    "    \"\"\"\n",
    "    Set SEED for PyTorch reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 44\n",
    "\n",
    "USE_SEED = False\n",
    "\n",
    "if USE_SEED:\n",
    "    set_seed(SEED, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b11b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, device, output_size=None):\n",
    "    \"\"\"Loads an image by transforming it into a tensor.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    output_dim = None\n",
    "    if output_size is None:\n",
    "        output_dim = (img.size[1], img.size[0])\n",
    "    elif isinstance(output_size, int):\n",
    "        output_dim = (output_size, output_size)\n",
    "    elif isinstance(output_size, tuple):\n",
    "        if (len(output_size) == 2) and isinstance(output_size[0], int) and isinstance(output_size[1], int):\n",
    "            output_dim = output_size\n",
    "    else:\n",
    "        raise ValueError(\"ERROR: output_size must be an integer or a 2-tuple of (height, width) if provided.\")\n",
    "\n",
    "    torch_loader = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(output_dim),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    img_tensor = torch_loader(img).unsqueeze(0)\n",
    "    return img_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbce125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Gloria´s paths\"\"\"\n",
    "#content_path = \"/home/gloria/Scrivania/Vision_and_cognitive_system/content_style/content.jpg\"\n",
    "#style_path = \"/home/gloria/Scrivania/Vision_and_cognitive_system/content_style/style1.jpg\"\n",
    "\n",
    "\"\"\"Sara´s paths\"\"\"\n",
    "content_path = \"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/taj_mahal.jpg\"\n",
    "style_path = \"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/vg_starry_night.jpg\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_size = 512\n",
    "\n",
    "content_tensor = load_image(content_path, device, output_size=output_size)\n",
    "output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
    "style_tensor = load_image(style_path, device, output_size=output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cc13b",
   "metadata": {},
   "source": [
    "# 2. Load VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "\n",
    "        #select 5 convolutional layers\n",
    "        self.chosen_features = {0: 'conv1_1', 5: 'conv2_1', 10: 'conv3_1', 19: 'conv4_1', 21: 'conv4_2', 28: 'conv5_1'}\n",
    "        self.vgg = torchvision.models.vgg19(weights='DEFAULT').features[:37]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature_maps = dict()\n",
    "        for idx, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            if idx in self.chosen_features.keys():\n",
    "                feature_maps[self.chosen_features[idx]] = x\n",
    "        \n",
    "        return feature_maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "vgg = VGG19().to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248352ba",
   "metadata": {},
   "source": [
    "# 3. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d21ea",
   "metadata": {},
   "source": [
    "The overall loss is constituted by the loss of the target image with respect to the content image, and the loss of the target image with respect to the style image. $$L_{tot}=L_{content}+L_{style}$$\n",
    "For this process it wouldn´t make sense to compare the images pixel by pixel: for example if the content image contains a house and the predominant style of the style image is to have diagonal lines, we would want the target image to be a house which is inclinated diagonally; comparing pixel by pixel an image with a diagonal house and an image with a house would return a much higher loss than we expect, because the pixel by pixel comparison doesn´t take into account more 'generic' features\n",
    "In order to perform a more accurate comparison, both these losses are evaluated between **feature maps** which take into account the more generic features of both images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f19ad7",
   "metadata": {},
   "source": [
    "### 3.1 Content loss\n",
    "The content loss is computed at the end of the CNN; we compute the mean squared error between the target feature map and the content feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(target_map, content_map):\n",
    "    #return torch.mean((content_original-content_current)**2)\n",
    "    return torch.nn.MSELoss(reduction='mean')(target_map, content_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ba0be",
   "metadata": {},
   "source": [
    "### 3.2 Style loss\n",
    "For the style loss, the procedure is more complicated.\n",
    "We are interested in co-occurrences of pairs of features to highlight important stylistic combinations.    \n",
    "\n",
    "If we have a feature map, of height and width $h,w$ and lenght $k$, which is the number of maps applied, we want to compute cooccurrences between each pair of  maps $i,j$ with values in $[0,k]$ range (these are also called *channels*): we obtain a $kxk$ matrix in which each entry is the dot product between two maps, a scalar.    \n",
    "Given the feature map of an image, this matrix, called the **Gram matrix** can be computed easily as the sum of the matrix multiplication between the whole feature map and its transpose.\n",
    "   \n",
    "This is done both with the feature map of the STYLE IMAGE and the feature map of the TARGET IMAGE.\n",
    "We compute a Gram matrix for both images for each convolutional layer considered $l$, and end up with:\n",
    "   - 5 Gram matrices of the style image feature maps $G_{style}^l$\n",
    "   - 5 Gram matrices of the target image feature maps  $G_{target}^l$    \n",
    "   \n",
    "The loss of each layer $l$ is computed via MSE between the two gram matrices, and the overall style loss will be the average of these values over the number of layers (in our case 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(target_map,style_map):\n",
    "    \"\"\"Compute MSE between gram matrix of style feature map and of generated feature map as style loss.\"\"\"\n",
    "    _, channel, height, width = target_map.shape\n",
    "    \n",
    "    #computing Gram matrix of the style feature map\n",
    "    style_gram = style_map.view(channel, height*width).mm(\n",
    "        style_map.view(channel, height*width).t()\n",
    "    )\n",
    "    #computing Gram matrix of the target feature map\n",
    "    target_gram = target_map.view(channel, height*width).mm(\n",
    "        target_map.view(channel, height*width).t()\n",
    "    )\n",
    "    # Normalize the Gram matrices\n",
    "    norm = channel * height * width\n",
    "    style_gram /= norm\n",
    "    target_gram /= norm\n",
    "    \n",
    "    return torch.mean((target_gram - style_gram) ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39e5da",
   "metadata": {},
   "source": [
    "### 3.3 Total variation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742da449",
   "metadata": {},
   "source": [
    "To regularize the loss function and encourage smoothness in the output image, we also introduce a total variation loss term to the total loss. This additional term will have to be weighted appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "    loss = torch.sum(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n",
    "           torch.sum(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33194d08",
   "metadata": {},
   "source": [
    "# 4. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddae890",
   "metadata": {},
   "source": [
    "### 4.1 Initialize random (target) image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=content_tensor\n",
    "img.shape \n",
    "\n",
    "#gaussian_noise_img = np.random.normal(loc=0, scale=90., size=img.shape).astype(np.float32)\n",
    "white_noise_img = np.random.uniform(-90., 90., img.shape).astype(np.float32)\n",
    "init_img = torch.from_numpy(white_noise_img).float().to(device)\n",
    "init_img = (init_img - init_img.min()) / (init_img.max() - init_img.min())\n",
    "init_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd02750",
   "metadata": {},
   "source": [
    "In this case, renormalized images are used in the CNN, but to visualize the actual images we have to denormalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c272970",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_show = transforms.ToPILImage()(torchvision.utils.make_grid(content_tensor.cpu()))\n",
    "style_show = transforms.ToPILImage()(torchvision.utils.make_grid(style_tensor.cpu()))\n",
    "rnd_show = transforms.ToPILImage()(torchvision.utils.make_grid(init_img.cpu()))\n",
    "\n",
    "\n",
    "# Display the original content, style image, and random noise images\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axs[0].imshow(content_show)\n",
    "axs[0].set_title(\"Content Image\")\n",
    "\n",
    "axs[1].imshow(style_show)\n",
    "axs[1].set_title(\"Style Image\")\n",
    "\n",
    "axs[2].imshow(rnd_show)\n",
    "axs[2].set_title(\"Random Noise Image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6336a44",
   "metadata": {},
   "source": [
    "###  4.2 Function to save intermediate feature maps\n",
    "Used during tests to check the behaviour of the feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68307707",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_content_features(content, model, layer_indices, intermediate_dir):\n",
    "    with torch.no_grad():\n",
    "        content_features = model(content)\n",
    "\n",
    "        for layer_idx in layer_indices:\n",
    "            layer_name = model.chosen_features[layer_idx]\n",
    "            feature_map = content_features[layer_name].squeeze(0)\n",
    "            \n",
    "            # Normalizza il tensore\n",
    "            normalized_feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min())\n",
    "            \n",
    "            # Converte la feature map in un'immagine utilizzando matplotlib\n",
    "            plt.imshow(normalized_feature_map[0].cpu().numpy(), cmap='viridis')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Salva l'immagine\n",
    "            save_path = os.path.join(intermediate_dir, f'content_{layer_idx}.jpg')\n",
    "            plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2895eb",
   "metadata": {},
   "source": [
    "### 4.2 Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b5df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = ['conv1_1','conv2_1','conv3_1','conv4_1','conv5_1']\n",
    "content_layers = ['conv4_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = content_tensor\n",
    "style = style_tensor\n",
    "target = init_img.requires_grad_(True)  #requires_grad is needed to make sure that the image is updated\n",
    "\n",
    "\n",
    "learn_rate=0.1\n",
    "alpha=5.0\n",
    "beta=1e7\n",
    "tv_weight=1e-3\n",
    "\n",
    "intermediate_dir=\"/home/sara/Scrivania/Physics_of_Data/2nd Year/Vision_cognitive_sys/Projects/neural_style_transfer/intermediate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05029f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image(content, style, target, device, output_img_fmt, content_img_name, style_img_name, num_epochs,\n",
    "               learn_rate):\n",
    "    \"\"\"Update the output image using pre-trained VGG19 model.\"\"\"\n",
    "    ...\n",
    "    \n",
    "    model = VGG19().to(device).eval()    # freeze parameters in the model\n",
    "\n",
    "    optimizer = torch.optim.Adam([target], lr=learn_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # get features maps of content, style and generated images from chosen layers\n",
    "        content_features = model(content)\n",
    "        style_features = model(style)\n",
    "        target_features = model(target)\n",
    "        \n",
    "        content_loss  = 0.0\n",
    "        style_loss= 0.0\n",
    "        \n",
    "        \n",
    "        \"\"\"Computing loss\"\"\"\n",
    "        for layer in target_features.keys(): \n",
    "            content_feature = content_features[layer]\n",
    "            style_feature = style_features[layer]\n",
    "            target_feature = target_features[layer]\n",
    "   \n",
    "            \n",
    "            if layer in content_layers:\n",
    "            # computes content loss on layer 4_2\n",
    "                content_loss_per_feature = get_content_loss(content_feature, target_feature)\n",
    "                content_loss += content_loss_per_feature\n",
    "                \n",
    "            if layer in style_layers:\n",
    "            # computes style loss for all 5 style layers\n",
    "                style_loss_per_feature = get_style_loss(style_feature, target_feature)\n",
    "                style_loss+=style_loss_per_feature\n",
    "        \n",
    "        #average style loss over all 5 layers\n",
    "        style_loss /= len(style_layers)\n",
    "        \n",
    "        tv_loss = total_variation_loss(target)\n",
    "        \n",
    "        # Total loss \n",
    "        total_loss = alpha * content_loss + beta * style_loss + tv_weight * tv_loss\n",
    "\n",
    "      #compute the gradient and update parameters\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        \n",
    "        #Save every 100 steps\n",
    "        if ((epoch+1)%50)==0:\n",
    "            save_image(target, os.path.join(intermediate_dir, f'nst-{content_img_name}-{style_img_name}-{epoch + 1}.{output_img_fmt}'))\n",
    "            \n",
    "            '''If we used the renormalization, we have to denormalize before saving for visualization purposes'''\n",
    "            #denormalized_target = denormalize(target.cpu().squeeze()).clamp(0, 1)\n",
    "            # Save the denormalized image\n",
    "            #save_image(denormalized_target, os.path.join(intermediate_dir, f'nst-denorm-{content_img_name}-{style_img_name}-{epoch + 1}.{output_img_fmt}'))\n",
    "\n",
    "\n",
    "        print(f\"\\tEpoch {epoch + 1}/{num_epochs}, loss = {total_loss}\") \n",
    "    ...\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f894b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_image(content, style, target, device,'jpeg', 'taj', 'vangogh',500,learn_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
